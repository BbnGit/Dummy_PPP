{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DNN For Elastic Property Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Initialize Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for initializing parameters for L-layer neural network\n",
    "\n",
    "def initialize_parameters(dim_layers):\n",
    "    \"\"\" \n",
    "    The arguments are,\n",
    "    dim_layers ---> A python array containing the dimensions of each layers in the network\n",
    "    \n",
    "    Function returs are,\n",
    "    parameters ---> A python dictionary containing the parameters of the network. i.e., Weights and bias in each layer\n",
    "                    W1,b1,W2,b2,.......WL,bL\n",
    "                    Where W1 : Weight matrix of shape (dim_layers[l], dim_layers[l-1])\n",
    "                    b1 : Bias vector of shape (dim_layers[l], 1)\n",
    "    \"\"\"\n",
    "    parameters = {}\n",
    "    L = len(dim_layers)   # total number of layers in the neural network\n",
    "    \n",
    "    for l in range(1, L):\n",
    "        parameters[\"W\" + str(l)] = np.random.randn(dim_layers[l], dim_layers[l-1]) * 0.01\n",
    "        parameters[\"b\" + str(l)] = np.zeros((dim_layers[l], 1))\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Forward propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Linear forward function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for linear forward\n",
    "\n",
    "def linear_forward(A, W, b):\n",
    "    \"\"\"\n",
    "    Arguments are,\n",
    "    A ---> Activations from the previous layer / input data\n",
    "    W ---> Weight matrix of shape (dim_layers[l], dim_layers[l-1])\n",
    "    b ---> Bias vector of shape (dim_layers[l], 1)\n",
    "    \n",
    "    Function returns are,\n",
    "    Z ---> Input for the activation function (Pre-activation parameter)\n",
    "    backup_values ---> for storing the values of A, W and b for Backward propagation - tuple\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    Z = np.dot(W,A) + b\n",
    "    backup_values = (A, W, b)\n",
    "    \n",
    "    return Z, backup_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Linear forward activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for linear forward activation\n",
    "\n",
    "def forward_activation(A_prev_layer, W, b, activation = \"relu\"):\n",
    "    \"\"\"\n",
    "    Arguments are,\n",
    "    A_prev_layer ---> Activation from the previous layer / input data\n",
    "    W --- > Weight matrix of shape (dim_layers[l], dim_layers[l-1])\n",
    "    b ---> Bias vector of shape (dim_layers[l], 1)\n",
    "    activation ---> Activation to be used (ReLu) i.e., A = RELU(Z) = max(0,Z)\n",
    "    \n",
    "    Function returns are,\n",
    "    A ---> Output of activation function (Post-activation parameter)\n",
    "    backup_values ---> for storing the values from linear forward calculation and activation function calculations\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    Z, linear_backup_values = linear_forward(A_prev_layer, W, b)\n",
    "    A, activation_backup_values = np.max(0, Z)\n",
    "    \n",
    "    backup_values = (linear_backup_values, activation_backup_values)\n",
    "    \n",
    "    return A, backup_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Forward propagation in neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for applying forward propagation in L layer network\n",
    "\n",
    "def L_layer_forward_prop(X, parameters):\n",
    "    \"\"\"\n",
    "    Arguments are,\n",
    "    X ---> Input data (numpy array)\n",
    "    parameters ---> A python dictionary containing the parameters of the network. i.e., Weights and bias in each layer\n",
    "                    Output of the function - initialize_parameters()\n",
    "                    \n",
    "    Function returns are,\n",
    "    A_L_layer ---> Activation from the output layer (L-th layer)\n",
    "    backup_values ---> for storing values of forward_activation()\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    A = X\n",
    "    backup_values_tot = []\n",
    "    L = len(parameters) // 2\n",
    "    \n",
    "    for l in range(1, L):\n",
    "        A_prev_layer = A\n",
    "        \n",
    "        A_L_layer, backup_values = forward_activation(A, parameters[\"W\" + str(l)], parameters[\"b\" + str(l)], activation = \"relu\")\n",
    "        backup_values_tot.append(backup_values)\n",
    "        \n",
    "    return A_L_layer, backup_values_tot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for calculating cost - J\n",
    "\n",
    "def cost(A_L_layer, Y):\n",
    "    \"\"\"\n",
    "    Arguments are,\n",
    "    A_L_layer ---> Vector corresponding to predictions of label with shape (1, number of examples)\n",
    "    Y ---> Label vector of shape (1, number of examples)\n",
    "    \n",
    "    Function returns are,\n",
    "    Cost ---> Cost of the predictions\n",
    "    \n",
    "    \"\"\"\n",
    "    m = Y.shape[1]\n",
    "    cost = -1/m * (np.dot(np.log(A_L_layer),Y.T) + np.dot(np.log(1-A_L_layer),(1-Y).T))\n",
    "    cost = np.squeeze(cost)\n",
    "                   \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Backward propagation in neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1 Linear backward function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for calculating linear backward\n",
    "\n",
    "def linear_backward(dZ, backup_values):\n",
    "    \"\"\"\n",
    "    Arguments are,\n",
    "    dZ ---> Gradient of the cost with respect to the linear output of each layer - dJ/dZ(l)\n",
    "    backup_values ---> Values of A_prev_layer, W and b from the forward propagation step - tuple\n",
    "    \n",
    "    Function returns are,\n",
    "    dW ---> Gradient of cost with respect to W\n",
    "    db ---> Gradient of cost with respect to b\n",
    "    dA_prev_layer ---> Gradient of cost with respect to the activation from the previous layer\n",
    "    \n",
    "    \"\"\"\n",
    "    A_prev_layer, W, b = backup_values\n",
    "    m = A_prev_layer.shape[1]\n",
    "    \n",
    "    dW = 1/m * np.dot(dZ, A_prev_layer.T)\n",
    "    db = 1/m * np.sum(dZ, axis = 1, keepdims = True)\n",
    "    dA_prev_layer = np.dot(W.T, dZ)\n",
    "    \n",
    "    return dA_prev_layer, dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 Linear backward activtion function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functon for backward step for activation\n",
    "\n",
    "def backward_activation(dA, backup_values, activation = \"relu\"):\n",
    "    \"\"\"\n",
    "    Arguments are,\n",
    "    dA ---> Activation gradient of the current layer\n",
    "    backup_values ---> Stored values of forward activation and linear step\n",
    "    activation ---> Activation to be used (ReLu) i.e., A = RELU(Z) = max(0,Z)\n",
    "    \n",
    "    Function returns are,\n",
    "    dA_prev_layer ---> Gradient of cost with respect to the activation of the previous layer\n",
    "    dW ---> Gradient of the cost with respect to W\n",
    "    db ---> Gradient of the cost with respect to b\n",
    "    \n",
    "    \"\"\"\n",
    "    (linear_backup_values, activation_backup_values) = backup_values\n",
    "    \n",
    "    dZ = dA * np.max(0,Z)\n",
    "    dA_prev_layer, dW, db = linear_backward(dZ, linear_backup_values)\n",
    "    \n",
    "    return dA_prev_layer, dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3 Backward propagation in neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for applying backward propagation in L layer network\n",
    "\n",
    "def L_layer_backward_prop(A_L_layer, Y, backup_values_tot):\n",
    "    \"\"\"\n",
    "    Arguments are,\n",
    "    A_L_layer ---> Vector corresponding to predictions - Output of forward propagation\n",
    "    Y ---> Label vector of shape (1, number of examples)\n",
    "    backup_values_tot ---> All backup_values of forward activation\n",
    "    \n",
    "    Function returns are,\n",
    "    gradients ---> Dictionary containing gradients i.e., dA, dW and db\n",
    "    \n",
    "    \"\"\"\n",
    "    gradients = {}\n",
    "    m = A_L_layer.shape[1]\n",
    "    Y = Y.reshape(A_L_layer.shape)\n",
    "    L = len(backup_values_tot)\n",
    "    \n",
    "    # initializing backpropagation\n",
    "    \n",
    "    dA_L_layer = -(np.divide(Y, A_L_layer) - np.divide(1-Y, 1-A_L_layer))\n",
    "    \n",
    "    # looping from l-1 to L=0\n",
    "    for l in reversed(range(L)):\n",
    "        current_value = backup_values_tot[l-1]\n",
    "        dA_prev_layer_tmp, dW_tmp, db_tmp = backward_activation(dA_prev_layer_tmp, current_value, activation = \"relu\")\n",
    "        gradients[\"dA\" + str(l-1)] = dA_prev_layer_tmp\n",
    "        gradients[\"dW\" + str(l)] = dW_tmp\n",
    "        gradients[\"db\" + str(l)] = db_tmp\n",
    "        \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Parameter updation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for updating parameters\n",
    "\n",
    "def update_params(params, gradients, learning_rate):\n",
    "    \"\"\"\n",
    "    Arguments are,\n",
    "    params ---> Dictionary of parameters\n",
    "    gradients ---> Dictionary of gradients\n",
    "    learning_rate ---> Determines the step size of each iterations\n",
    "    \n",
    "    Function returns are,\n",
    "    parametrs ---> Dictionary containing the updated parameters\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    parameters = params.copy()\n",
    "    L = len(parameters) // 2 \n",
    "    \n",
    "    for l in range(L):\n",
    "        \n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * gradients[\"dW\" + str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * gradients[\"db\" + str(l+1)]\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
